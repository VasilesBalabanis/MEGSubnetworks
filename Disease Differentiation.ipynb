{
 "cells": [
{
   "cell_type": "markdown",
   "id": "ab742074",
   "metadata": {},
   "source": [
    "- [scikit-learn](https://scikit-learn.org/) - BSD 3-Clause License
- [seaborn](https://seaborn.pydata.org/) - BSD 3-Clause License
- [matplotlib](https://matplotlib.org/) - PSF License (BSD 3-Clause compatible)
- [TensorFlow](https://www.tensorflow.org/) - Apache License 2.0
- [NumPy](https://numpy.org/) - BSD 3-Clause License.
LICENSE: This file is part of the project MEGSubnetworks. All code in MEGSubnetworks is free: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License on this link. You should have received a copy of the GNU General Public License along with MEGSubnetworks. If not, see https://www.gnu.org/licenses/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe388d9",
   "metadata": {},
   "source": [
    "## Stacked Autoencoder for Disease Differentiation using sub-networks in the MEG Functional Connectome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9ccb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab742073",
   "metadata": {},
   "source": [
    "Loading healthy and diseased functional connectomes in form (30,116,116). 30 is the number of subjects and 116x116 is the regions in the functional connectome. These values do not matter, feel free to use any atlas or number of subjects. (Note: you will need to edit code in the deep learning part to address a different number of subjects or different number of regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3d660c",
   "metadata": {},
   "outputs": [],
   "source": [
    "diseasedFC = np.load('diseasedFC.npy', allow_pickle = True)\n",
    "healthyFC = np.load('healthyFC.npy', allow_pickle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ca2ffa",
   "metadata": {},
   "source": [
    "This gets the non-zero indices from the upper-triangle, for later extraction of the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c504d7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = healthyFC[0,:,:]\n",
    "upper_triangular = np.triu(correlation_matrix, k=1)\n",
    "upper_triangular[np.diag_indices_from(upper_triangular)] = 0\n",
    "nonzero_indices = np.nonzero(upper_triangular)\n",
    "nonZIndices = nonzero_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48498f92",
   "metadata": {},
   "source": [
    "This should be your sub-networks file, which is a numpy of lists of optimal sub-networks by their region indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6310ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_subnetworks = np.load('optimalSubnetworksAll.npy', allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edf91bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loaded_subnetworks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1fe4e2",
   "metadata": {},
   "source": [
    "This is a function that extracts sub-network specific non-zero indices from the functional connectome. (If you want to do a null-distribution, simply enable the 'allowed_values' first line and make sure to change the number of regions you want to sub-network as 'index' in the next block of code. Change the range from 90 to however many regions in your atlas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc7f16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractSubNetworks(allowed_values, index):\n",
    "    #allowed_values = np.random.choice(range(90), index, replace=False).tolist()\n",
    "    #print(allowed_values)\n",
    "    a = nonZIndices[0]\n",
    "    b = nonZIndices[1]\n",
    "    mask = np.isin(a, allowed_values) & np.isin(b, allowed_values)\n",
    "    filtered_a = a[mask]\n",
    "    filtered_b = b[mask]\n",
    "\n",
    "    filtered_tuple = (filtered_a, filtered_b)\n",
    "    return filtered_tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b002ce",
   "metadata": {},
   "source": [
    "This gets the data ready in a ready format, where sub-networks are saved per individual. Note: it requires the same number of diseased and healthy subjects in the for loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41301b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "brainNetworkHC = []\n",
    "brainNetworkParkinsons = []\n",
    "\n",
    "for k in range(len(loaded_subnetworks)):\n",
    "  print(\"index:\",k)\n",
    "  trainHC = []\n",
    "  trainParkinsons = []\n",
    "\n",
    "  nonZIndicesToUse = extractSubNetworks(loaded_subnetworks[k], 10)\n",
    "\n",
    "  for i in range(len(diseasedFC)):\n",
    "      val2 = healthyFC[i]\n",
    "      val12 = diseasedFC[i]\n",
    "      val2Features = val2[nonZIndicesToUse]\n",
    "      val12Features = val12[nonZIndicesToUse]\n",
    "        \n",
    "      trainHC.append(val2Features)\n",
    "      trainParkinsons.append(val12Features)\n",
    "\n",
    "  brainNetworkHC.append(trainHC)\n",
    "  brainNetworkParkinsons.append(trainParkinsons)\n",
    "    \n",
    "checkTrainHCBrain = np.array(brainNetworkHC)\n",
    "checkTrainParkinsonsBrain = np.array(brainNetworkParkinsons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46bf967",
   "metadata": {},
   "source": [
    "This will plot a confusion matrix every time a model finishes running. This will show how many sub-networks are correctly predicted, cumulative across all subjects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4ff4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap='mako'):\n",
    "    if not title:\n",
    "        title = 'Confusion matrix'\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='g', cmap=cmap,\n",
    "                xticklabels=classes, yticklabels=classes, cbar=False)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f470cc70",
   "metadata": {},
   "source": [
    "### Deep learning: Stacked Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d94442c",
   "metadata": {},
   "source": [
    "It first iterates across numbers of sub-networks to include in the learning. Sub-network correlation values are standardized using StandardScaler() which makes it zero-mean and unit variance. Fold ranges are designed for 30 healthy and 30 diseased, concatenated together. This does 6-fold cross-validation, which ensures 5 from the healthy and 5 from the diseased are test sets for training. If you want to change it, you can edit it accordingly. Try to keep cross-validation sample sizes the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107bd11f",
   "metadata": {},
   "source": [
    "For the model itself, though unlikely, it may face version conflicts. Our versions used were Python version: 3.10.9, TensorFlow version: 2.16.1, scikit-learn version: 1.2.1, numpy version: 1.23.5.\n",
    "\n",
    "Check below to see yours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0533c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Python version:\", sys.version.split(\" \")[0])\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"scikit-learn version:\", sklearn.__version__)\n",
    "print(\"NumPy version:\", np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6347ed16",
   "metadata": {},
   "source": [
    "The model architecture is available on the Readme file. If you want to understand deep learning better, please look into it yourself. The final predictions of every model are saved and organized in a numpy of (6,6), representing cross-validations and feature-subsets. The test accuracies per epoch are also saved, if you would like to look at convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5fd3e5",
   "metadata": {},
   "source": [
    "This model was not thoroughly optimized. I ran this without a GPU. You may need to do a configuration to work with GPU. Please feel free to explore different model approaches. Performance is likely to increase..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fb9d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_subsets = [1000, 2000, 5000, 10000, 20000, 40000]\n",
    "\n",
    "biggerAccuracyHistories = []\n",
    "biggerPrecisionHistories = []\n",
    "biggerRecallHistories = []\n",
    "biggerF1Histories = []\n",
    "bigYPred = []\n",
    "for feature_length in feature_subsets:\n",
    "    print(\"Feature length:\",feature_length)\n",
    "    \n",
    "    newcheckTrainHCBrain = checkTrainHCBrain[:feature_length, :, :]\n",
    "    newcheckTrainParkinsonsBrain = checkTrainParkinsonsBrain[:feature_length, :, :]\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Change these values depending on how many subjects you have and how many values in your sub-network.\n",
    "    Here, it is 45 values from 10x10 regions, and 30 Parkinson's patients and 30 healthy control.\n",
    "    We assign 1 for healthy, and 0 for Parkinson's\n",
    "    '''\n",
    "    \n",
    "    new_shape = (30*feature_length, 45)\n",
    "\n",
    "    intVal = 30*feature_length\n",
    "    \n",
    "    X = np.concatenate((newcheckTrainHCBrain.reshape(new_shape), \n",
    "                    newcheckTrainParkinsonsBrain.reshape(new_shape)), axis=0)\n",
    "\n",
    "    y = np.concatenate((np.ones(intVal), np.zeros(intVal)),axis=0)\n",
    "\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    best_validation_accuracies = []\n",
    "    accuracy_histories = []\n",
    "    precision_histories = []\n",
    "    recall_histories = []\n",
    "    f1_histories = []\n",
    "    YPred = []\n",
    "    fold_no = 1\n",
    "\n",
    "    input_dim = X_scaled.shape[1]\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Change these fold values, depending on how many subjects you have and how you want to cross-validate. Keep consistent with above\n",
    "    '''\n",
    "    \n",
    "    fold_ranges = [\n",
    "        (0, 5, 30, 35), \n",
    "        (5, 10, 35, 40),\n",
    "        (10, 15, 40, 45),\n",
    "        (15, 20, 45, 50),\n",
    "        (20, 25, 50, 55),\n",
    "        (25, 30, 55, 60)\n",
    "    ]\n",
    "\n",
    "\n",
    "    num_samples_per_class = feature_length\n",
    "    total_samples = X_scaled.shape[0]\n",
    "\n",
    "    for fold_no, (start_1, end_1, start_2, end_2) in enumerate(fold_ranges, start=1):\n",
    "        \n",
    "        test_indices_1 = np.arange(start_1 * num_samples_per_class, end_1 * num_samples_per_class)\n",
    "        test_indices_2 = np.arange(start_2 * num_samples_per_class, end_2 * num_samples_per_class)\n",
    "\n",
    "        test_indices = np.concatenate([test_indices_1, test_indices_2])\n",
    "        train_indices = np.setdiff1d(np.arange(total_samples), test_indices)\n",
    "\n",
    "\n",
    "        X_train, X_test = X_scaled[train_indices], X_scaled[test_indices]\n",
    "        y_train, y_test = y[train_indices], y[test_indices]\n",
    "        X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "\n",
    "        input_layer = Input(shape=(input_dim,))\n",
    "        encoded = Dense(512, activation='relu')(input_layer)\n",
    "        encoded = BatchNormalization()(encoded)\n",
    "        encoded = Dropout(0.3)(encoded)\n",
    "        encoded = Dense(256, activation='relu', activity_regularizer=l2(1e-4))(encoded)\n",
    "\n",
    "        decoded = Dense(256, activation='relu')(encoded)\n",
    "        decoded = BatchNormalization()(decoded)\n",
    "        decoded = Dropout(0.3)(decoded)\n",
    "        decoded = Dense(input_dim, activation='sigmoid', name='decoder_output')(decoded)\n",
    "\n",
    "        classifier = Dense(128, activation='relu')(encoded)\n",
    "        classifier = Dropout(0.3)(classifier)\n",
    "        classifier = Dense(1, activation='sigmoid', name='classifier_output')(classifier)\n",
    "\n",
    "\n",
    "        autoencoder = Model(inputs=input_layer, outputs=[decoded, classifier])\n",
    "\n",
    "        autoencoder.compile(optimizer='adam',\n",
    "                            loss={'decoder_output': 'mse', 'classifier_output': 'binary_crossentropy'},\n",
    "                            metrics={'classifier_output': 'accuracy'})\n",
    "\n",
    "        print(f'Training for fold {fold_no} ...')\n",
    "        history = autoencoder.fit(X_train, {'decoder_output': X_train, 'classifier_output': y_train}, validation_data=(\n",
    "            X_test, {'decoder_output': X_test, 'classifier_output': y_test}), epochs=100, batch_size=128, verbose=1)\n",
    "\n",
    "        \n",
    "        y_pred_probs = autoencoder.predict(X_test)[1]\n",
    "        y_pred = (y_pred_probs > 0.5).astype(\"int32\") \n",
    "        YPred.append(y_pred)\n",
    "\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        \n",
    "        precision_histories.append(precision)\n",
    "        recall_histories.append(recall)\n",
    "        f1_histories.append(f1)\n",
    "        plot_confusion_matrix(y_test, y_pred, classes=['Healthy Control', 'Parkinson'], title='Confusion Matrix')\n",
    "        \n",
    "        print(f\"Precision for fold {fold_no}: {precision:.4f}\")\n",
    "        print(f\"Recall for fold {fold_no}: {recall:.4f}\")\n",
    "        print(f\"F1 Score for fold {fold_no}: {f1:.4f}\")\n",
    "        \n",
    "        best_val_acc = max(history.history['val_classifier_output_accuracy'])\n",
    "        best_validation_accuracies.append(best_val_acc)\n",
    "\n",
    "        print(f\"Best validation accuracy for fold {fold_no}: {best_val_acc:.4f}\")\n",
    "\n",
    "        accuracy_histories.append(history.history['val_classifier_output_accuracy'])\n",
    "\n",
    "        fold_no += 1\n",
    "    biggerAccuracyHistories.append(accuracy_histories)\n",
    "    biggerPrecisionHistories.append(precision_histories)\n",
    "    biggerRecallHistories.append(recall_histories)\n",
    "    biggerF1Histories.append(f1_histories)\n",
    "    bigYPred.append(YPred)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, history in enumerate(accuracy_histories, 1):\n",
    "        plt.plot(history, label=f'Fold {i}')\n",
    "    plt.title('Validation Accuracy for Each Fold with feature length '+str(feature_length))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25d309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predToSave = np.array(bigYPred, dtype = object)\n",
    "np.save('autoencoderSubnetworkPredictions.npy', predToSave)\n",
    "bigAccuracyToSave = np.array(biggerAccuracyHistories, dtype = object)\n",
    "np.save('autoencoderSubnetworkEpochAccuracies.npy', bigAccuracyToSave)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
